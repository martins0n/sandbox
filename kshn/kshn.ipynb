{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiHseHVQJJsl",
        "outputId": "1bc54857-9dfa-41a9-be6f-433e68def7a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sandbox'...\n",
            "remote: Enumerating objects: 372, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 372 (delta 48), reused 57 (delta 37), pack-reused 293\u001b[K\n",
            "Receiving objects: 100% (372/372), 117.36 MiB | 17.16 MiB/s, done.\n",
            "Resolving deltas: 100% (136/136), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/martins0n/sandbox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd sandbox && git checkout kshn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-0pbetiJLJY",
        "outputId": "f16cbf4c-b32d-405a-dd6d-96d098e9a03e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Branch 'kshn' set up to track remote branch 'kshn' from 'origin'.\n",
            "Switched to a new branch 'kshn'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd sandbox/kshn && pip install -U -r requirements.txt 1>/dev/null 2>/dev/null"
      ],
      "metadata": {
        "id": "gsQZHUkEJc-L"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd sandbox/kshn/model && bash train_gpt.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EDWBgalJjYO",
        "outputId": "2a9c6086-285d-45a3-91f6-20a0abae9686"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2022 10:37:38 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "09/03/2022 10:37:38 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=../tmp/test-clm/runs/Sep03_10-37-37_0c78850e58c0,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=epoch,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=5.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=../tmp/test-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=../tmp/test-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=epoch,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "09/03/2022 10:37:39 - WARNING - datasets.builder - Using custom data configuration default-f5eea56fc2aeef67\n",
            "09/03/2022 10:37:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "09/03/2022 10:37:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253\n",
            "09/03/2022 10:37:39 - WARNING - datasets.builder - Reusing dataset json (/root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n",
            "09/03/2022 10:37:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253\n",
            "100% 2/2 [00:00<00:00, 185.73it/s]\n",
            "[INFO|configuration_utils.py:681] 2022-09-03 10:37:40,532 >> loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "[INFO|configuration_utils.py:730] 2022-09-03 10:37:40,533 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:404] 2022-09-03 10:37:41,496 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:681] 2022-09-03 10:37:42,459 >> loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "[INFO|configuration_utils.py:730] 2022-09-03 10:37:42,460 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-09-03 10:37:49,017 >> loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/1b36eeb1fd7b3a6ec11bf46bde2c38e7e68f71ec774694b9e886c86001aab35d.c483bc3440d25937fdac74506b73b76ee6e67f778a804756214363fc2a1a66ef\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-09-03 10:37:49,018 >> loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/479aa59074c4dcd4c36106252da033d03bc92e3010947ce1d3714de224c2af1f.7362c0dbb32f750eeea5a5b93bbd0c6876eac41453369265d5a49df1c9142b6f\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-09-03 10:37:49,018 >> loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-09-03 10:37:49,018 >> loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-09-03 10:37:49,018 >> loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1803] 2022-09-03 10:37:49,018 >> loading file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:681] 2022-09-03 10:37:49,946 >> loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "[INFO|configuration_utils.py:730] 2022-09-03 10:37:49,947 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:681] 2022-09-03 10:37:50,982 >> loading configuration file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/06f48b6b3173390d047e15d691fda67ae4ea7733a5eea4b6e0115f5099c4e700.b5cdfa39c63384f94159c36bc9042660c747cea5cf520b43d543bd2c68b3164d\n",
            "[INFO|configuration_utils.py:730] 2022-09-03 10:37:50,983 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"sberbank-ai/rugpt3small_based_on_gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.21.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:279] 2022-09-03 10:37:51,055 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:2041] 2022-09-03 10:37:52,007 >> loading weights file https://huggingface.co/sberbank-ai/rugpt3small_based_on_gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/df2b64a4c86a349ba84354d85b7117b106f2b87085c9bb54cde70d3751907c45.4e3da19dd8adaa6d6a9804bfd45d2dcf17ba544de445847443ef1816bfa3d693\n",
            "[INFO|modeling_utils.py:2435] 2022-09-03 10:37:56,709 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2444] 2022-09-03 10:37:56,710 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at sberbank-ai/rugpt3small_based_on_gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "09/03/2022 10:37:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-52e11c70187e5c1a.arrow\n",
            "09/03/2022 10:37:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-5f47ea7858156a82.arrow\n",
            "09/03/2022 10:37:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-23c7d2cdd2719360.arrow\n",
            "09/03/2022 10:37:57 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-f5eea56fc2aeef67/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-b15708cc1434004d.arrow\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1605] 2022-09-03 10:38:01,920 >> ***** Running training *****\n",
            "[INFO|trainer.py:1606] 2022-09-03 10:38:01,920 >>   Num examples = 1250\n",
            "[INFO|trainer.py:1607] 2022-09-03 10:38:01,920 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:1608] 2022-09-03 10:38:01,920 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1609] 2022-09-03 10:38:01,920 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1610] 2022-09-03 10:38:01,920 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1611] 2022-09-03 10:38:01,921 >>   Total optimization steps = 3125\n",
            "{'loss': 3.8371, 'learning_rate': 4e-05, 'epoch': 1.0}\n",
            " 20% 625/3125 [01:15<05:04,  8.21it/s][INFO|trainer.py:2640] 2022-09-03 10:39:17,170 >> Saving model checkpoint to ../tmp/test-clm/checkpoint-625\n",
            "[INFO|configuration_utils.py:451] 2022-09-03 10:39:17,171 >> Configuration saved in ../tmp/test-clm/checkpoint-625/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-09-03 10:39:18,990 >> Model weights saved in ../tmp/test-clm/checkpoint-625/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-09-03 10:39:18,991 >> tokenizer config file saved in ../tmp/test-clm/checkpoint-625/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-09-03 10:39:18,992 >> Special tokens file saved in ../tmp/test-clm/checkpoint-625/special_tokens_map.json\n",
            "{'loss': 3.1238, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
            " 40% 1250/3125 [02:38<03:49,  8.19it/s][INFO|trainer.py:2640] 2022-09-03 10:40:40,520 >> Saving model checkpoint to ../tmp/test-clm/checkpoint-1250\n",
            "[INFO|configuration_utils.py:451] 2022-09-03 10:40:40,521 >> Configuration saved in ../tmp/test-clm/checkpoint-1250/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-09-03 10:40:42,279 >> Model weights saved in ../tmp/test-clm/checkpoint-1250/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-09-03 10:40:42,280 >> tokenizer config file saved in ../tmp/test-clm/checkpoint-1250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-09-03 10:40:42,281 >> Special tokens file saved in ../tmp/test-clm/checkpoint-1250/special_tokens_map.json\n",
            "{'loss': 2.6258, 'learning_rate': 2e-05, 'epoch': 3.0}\n",
            " 60% 1875/3125 [04:01<02:35,  8.04it/s][INFO|trainer.py:2640] 2022-09-03 10:42:03,846 >> Saving model checkpoint to ../tmp/test-clm/checkpoint-1875\n",
            "[INFO|configuration_utils.py:451] 2022-09-03 10:42:03,847 >> Configuration saved in ../tmp/test-clm/checkpoint-1875/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-09-03 10:42:05,759 >> Model weights saved in ../tmp/test-clm/checkpoint-1875/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-09-03 10:42:05,760 >> tokenizer config file saved in ../tmp/test-clm/checkpoint-1875/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-09-03 10:42:05,760 >> Special tokens file saved in ../tmp/test-clm/checkpoint-1875/special_tokens_map.json\n",
            "{'loss': 2.2578, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
            " 80% 2500/3125 [05:25<01:17,  8.09it/s][INFO|trainer.py:2640] 2022-09-03 10:43:27,672 >> Saving model checkpoint to ../tmp/test-clm/checkpoint-2500\n",
            "[INFO|configuration_utils.py:451] 2022-09-03 10:43:27,673 >> Configuration saved in ../tmp/test-clm/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-09-03 10:43:29,376 >> Model weights saved in ../tmp/test-clm/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-09-03 10:43:29,378 >> tokenizer config file saved in ../tmp/test-clm/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-09-03 10:43:29,378 >> Special tokens file saved in ../tmp/test-clm/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 2.016, 'learning_rate': 0.0, 'epoch': 5.0}\n",
            "100% 3125/3125 [06:49<00:00,  8.15it/s][INFO|trainer.py:2640] 2022-09-03 10:44:51,316 >> Saving model checkpoint to ../tmp/test-clm/checkpoint-3125\n",
            "[INFO|configuration_utils.py:451] 2022-09-03 10:44:51,317 >> Configuration saved in ../tmp/test-clm/checkpoint-3125/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-09-03 10:44:53,098 >> Model weights saved in ../tmp/test-clm/checkpoint-3125/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-09-03 10:44:53,100 >> tokenizer config file saved in ../tmp/test-clm/checkpoint-3125/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-09-03 10:44:53,100 >> Special tokens file saved in ../tmp/test-clm/checkpoint-3125/special_tokens_map.json\n",
            "[INFO|trainer.py:1850] 2022-09-03 10:44:57,158 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 415.2376, 'train_samples_per_second': 15.052, 'train_steps_per_second': 7.526, 'train_loss': 2.7720955078125, 'epoch': 5.0}\n",
            "100% 3125/3125 [06:55<00:00,  7.52it/s]\n",
            "[INFO|trainer.py:2640] 2022-09-03 10:44:57,335 >> Saving model checkpoint to ../tmp/test-clm\n",
            "[INFO|configuration_utils.py:451] 2022-09-03 10:44:57,344 >> Configuration saved in ../tmp/test-clm/config.json\n",
            "[INFO|modeling_utils.py:1566] 2022-09-03 10:44:59,472 >> Model weights saved in ../tmp/test-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2145] 2022-09-03 10:44:59,473 >> tokenizer config file saved in ../tmp/test-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2152] 2022-09-03 10:44:59,473 >> Special tokens file saved in ../tmp/test-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        5.0\n",
            "  train_loss               =     2.7721\n",
            "  train_runtime            = 0:06:55.23\n",
            "  train_samples            =       1250\n",
            "  train_samples_per_second =     15.052\n",
            "  train_steps_per_second   =      7.526\n",
            "09/03/2022 10:44:59 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2891] 2022-09-03 10:45:00,008 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2893] 2022-09-03 10:45:00,008 >>   Num examples = 146\n",
            "[INFO|trainer.py:2896] 2022-09-03 10:45:00,008 >>   Batch size = 2\n",
            "100% 73/73 [00:02<00:00, 33.95it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =     0.3414\n",
            "  eval_loss               =     4.2819\n",
            "  eval_runtime            = 0:00:02.15\n",
            "  eval_samples            =        146\n",
            "  eval_samples_per_second =     67.607\n",
            "  eval_steps_per_second   =     33.803\n",
            "  perplexity              =    72.3787\n",
            "[INFO|modelcard.py:467] 2022-09-03 10:45:03,172 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.34138712113040665}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model='/content/sandbox/kshn/tmp/test-clm'\n",
        ")"
      ],
      "metadata": {
        "id": "5Fx2e7fSKWq7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "premise = [\n",
        "    'Слышно ли меня?',\n",
        "    'Навальный - это',\n",
        "    'Путин - это',\n",
        "    'Учите санскрит',\n",
        "    'Мурзилка Венедиктов',\n",
        "    'Химкинский лес',\n",
        "    'Лондонская погода',\n",
        "    'Всем привет! Я - Олег Кашин и',\n",
        "    'Олег Кашин произносит краткую речь',\n",
        "    'Возможно, лучший стрим в этом жанре.',\n",
        "    'Приморские партизаны',\n",
        "    'Эхо Москвы',\n",
        "    'Старая площадь'\n",
        "]\n",
        "\n",
        "generator_output = generator(\n",
        "    premise, num_beams=3, max_length=128,\n",
        "    repetition_penalty=5.0,\n",
        "    early_stopping=True,\n",
        "    temperature=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeHMFy8COJRt",
        "outputId": "eb452d5c-f9b8-4d6f-ac99-2fa31cc7e777"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for gen in generator_output:\n",
        "  print(gen[0]['generated_text'])"
      ],
      "metadata": {
        "id": "Z8T524USOfDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedf2aa8-5135-49c0-9341-07484bc28d9d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слышно ли меня? Нет, не слышу. А что слышно-то?\n",
            "Илья Эренбург: Да все как всегда — и «мурзилок», и «болотное дело». И вот я сейчас нахожусь в такой ситуации, что у меня нет другого выбора, кроме как согласиться с тем, что Болотная площадь — это просто очередной виток эволюционной борьбы за место на скамье подсудимых. Я прекрасно понимаю, что если бы я соглашался с Навальным, то мне бы грозил реальный тюремный срок или даже пять лет лишения свободы. Но ведь есть же альтернатива — например, условный сбыт\n",
            "Навальный - это не оппозиция, Навальный всегда был и остается оппозиционером номер один. Это единственная форма политической борьбы в современной России, которая возможна вне зависимости от формальных политических партий и убеждений граждан. Политическая борьба невозможна вне зависимости от формальных политических партий и убеждений граждан. Если сегодня ты поддерживаешь Навального, то завтра ты будешь поддерживать и его тоже. И если завтра ты поддерживаешь Удальцова, то завтра ты поддерживаешь и его тоже. Полностью Олег КАШИН, «Кашин», специально для «Кашина» На прошлой неделе я писал о том, что на сайте Кремля появилась информация о сборе компромата на\n",
            "Путин - это не тот человек, которого принято называть «быдлом», и даже если бы он действительно был таким, как все, его заслуги в сегодняшней России бесспорны. Полностью Олег КАШИН, «Кашин» На прошлой неделе я писал о том, что мой новый знакомый Илья Варламов (он же Slon), известный также под ником Pussy Riot, выступил с заявлением по поводу недопуска российских журналистов на Болотную площадь 6 мая 2012 года. Это заявление было сделано после того, как двое мужчин из группы \"Ария\" во время акции у ОВД \"Китай-город\" скандировали \"Поз\n",
            "Учите санскрит, а не то мы вас всех пересажаем на органы!»\n",
            "Илья Эренбург в «Комсомольской правде» писал тогда: «В дни торжеств по случаю 100-летия со дня рождения великого русского писателя и общественного деятеля В. И. Ленина у памятника Ленину будет стоять памятник его создателю — величайшему русскому писателю второй половины XIX века».  Памятник Владимиру Ильичу Ленину появится через два года – к этому времени уже будут отреставрированы многие ленинские памятники. На месте снесенного пятиэтажки построят торговый центр с кофейней «Ленинец», кинотеатр\n",
            "Мурзилка Венедиктовна — это не более чем пиар-акции, которые направлены на то, чтобы отвлечь внимание общественности от реальных политических событий. Это единственная форма публичного издевательства над здравым смыслом и ценностями. Если бы я был депутатом Государственной думы второго или третьего созывов, мне бы пришлось доказывать, что я имею в виду именно эти два случая, а не какие-то совсем абстрактные эпизоды моей депутатской биографии. Я искренне надеюсь, что хотя бы часть этих эпизодов будет проигнорирована Кремлем через какое-то время, потому что иначе мы просто не узнаем правды о том времени, которое нам отпущено\n",
            "Химкинский лес, или как там его теперь принято называть — Химкинский хребет, это примерно то же самое, что Чечня. Это очень длинная история, и я хотел бы обратить ваше внимание на один важный момент. Дело в том, что когда Грозный захватывают федеральные телеканалы, они транслируют всю ту мерзость, которую показывают по федеральным телеканалам. То есть если вы смотрели «Бронзового солдата» с Леонидом Якубовичем, вам не приходило в голову, что надо было смотреть «Касабланку», а не «Утомленные солнцем». В этом смысле у нас все просто замечательно\n",
            "Лондонская погода» — это не более чем предвыборная агитация, замаскированная под настоящую борьбу за место в Государственной Думе. Партия «Яблоко» давно превратилась в сборище политических демагогов и провокаторов, готовых на любое гнусное дело ради сохранения власти своих ставленников во главе с Кремлем. Сегодняшние политические демагоги и провокаторы уже не скрывают своих симпатий к Западу, а открыто говорят о том, что готовы пойти на сделку со стерхами (согласно популярному шуточному поверью, если женщина забеременеет от своего партнера, то у нее родится ребенок). Лидеров\n",
            "Всем привет! Я - Олег Кашин и автор блога «Кашин.ру» с 2007 года. В этом блоге я пишу о том, чего не было на самом деле — о журналистике в России; о современной российской журналистике, которая существует только благодаря моему трудолюбию и упорству. Если вы заметили какие-то незначительные ошибки или недочёты в статьях или текстах, пожалуйста, пишите об этом в комментариях. Ваши ответы будут опубликованы после проверки модератором. Редакция надеется, что их количество будет достаточным для того, чтобы исправить обнаруженные опечатки или недочёты. С Уважением, Александр Коц. Незы\n",
            "Олег Кашин произносит краткую речь, посвященную Дню космонавтики. Полностью Олег Кашин — российский политический и общественный деятель, автор более 40 публикаций в российских СМИ (в том числе «Кашин против путинского режима», «Новая газета», «Независимая газета»), главный редактор радиостанции «Шансон» и ряда информационных агентств. В свободное от политики время Кашин активно участвует в общественной жизни Москвы: он входит в совет директоров компании «Ренова», является членом правления фонда «Национальное интеллектуальное развитие». Известность Кашину принесли сериалы «Универ», «Игры престолов», «Горби-дрим\n",
            "Возможно, лучший стрим в этом жанре. Я не уверен, что это удачный пример, но мне нравится, как меняется интонация у людей, когда они говорят «да» или «нет».  Полностью Олег КАШИН, «Кашин», специально для «Кашина» На прошлой неделе я написал пост о том, что меня больше всего раздражает то, что многие комментаторы относятся к моим текстам с опаской — ну вот хотя бы из-за того, что я пишу тексты на очень похожую тему, а их много. То есть если кто-то вдруг решит относиться ко мне со скепсисом, стоит быть готовыми\n",
            "Приморские партизаны, которые вброд через пролив Босфор Восточный идут воевать против турецкого владычества — это все-таки слишком фантастично и неправдоподобно, чтобы всерьез относиться к таким историческим фактам. Но даже если отбросить фантазию о том, что на востоке Украины воюют не только сепаратисты, но и бандеровцы, то есть те же самые люди, которых мы привыкли видеть героями украинских событий девяностых или хотя бы позапрошлогодних, то есть те же самые люди, которым за тридцать, а им за сорок, и у них своя собственная историческая родина, которая их кормит и одевает, и\n",
            "Эхо Москвы» и «Комсомольской правды», то есть все, что нужно знать о журналистике в современной России. Полностью Олег КАШИН, «Кашин» На прошлой неделе мы с моим коллегой Дмитрием Коваленко разобрали популярную социальную сеть Инстаграм Олега Кашина (он там теперь главный редактор). Оказалось, что он давно дружит с Евгенией Альбац, известной также под ником Pussy Riot. Мы спросили его об этом, не хочет ли он познакомиться с ней поближе. Он ответил: «Не знаю, как вы, а я бы хотел познакомиться». Я спросил его, может ли он\n",
            "Старая площадь» — это не более чем предвыборная агитация, замаскированная под настоящую стахановскую стахановскую партию. Стахановская партия существует только для того, чтобы в ней заседало очередное партийное совещание или конференция с участием представителей власти и крупного капитала. На самом деле на этой площади собрались сотни тысяч людей, готовых отдать свои голоса за любую перемену к лучшему. И если завтра случится что-нибудь совсем плохое, то тысячи этих людей обязательно поддержат новую власть ради нее самой. «Мы отдаем себе отчет в том, что перемены неизбежны», — торжественно объявил первый секретарь обкома КПСС Геннадий Андреевич Зюганов перед собрав\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0cdYgGGx_lh-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}